###Get the data
churndata <- read.csv("churndata.csv")
head(churndata)
str(churndata)
###Prepare the data: Clean up, check the data types
churndata$churn <- factor(churndata$churn)
churndata$vmail <- factor(churndata$vmail)
churndata$area <- factor(churndata$area)
sum(is.na(churndata))
churndata <- na.omit(churndata)
## Scale the numerical predictors
churndata[c(4:17)] <- scale(churndata[c(4:17)])
###Create training and test sets
set.seed(123)
churndata <- mutate(churndata, id=1:nrow(churndata))
t_prop <- 0.7
train <- slice_sample(churndata, n = t_prop*nrow(churndata), replace=FALSE)
test <- anti_join(churndata, train, by = 'id')
train <- select(train, -id)
test <- select(test, -id)
set.seed(seed=NULL) # release the seed and set it to be random
### glm function takes the following inputs
### 1. dependent variable ~ independent variables
### 2. data = dataset (whichever data set you would like to build the model with)
### 3. family = binomial
glm.fit <- glm(churn ~ ., data = train, family = binomial)
summary(glm.fit)
### Convert the predicted probabilities into class labels No or Yes
glm.pred <- rep("No", nrow(test))
### type="response" outputs probabilities P(Y = 1|X) instead of the log-odds (logit).
glm.probs <- predict(glm.fit, newdata = test, type = "response")
glm.pred[glm.probs > .5] <- "Yes"
### Create the confusion matrix and calculate error rates
glm.table <- table(test$churn, glm.pred, dnn = c("Actual", "Predicted"))
glm.table
### Overall error rate
(glm.table[1, 2]+glm.table[2, 1])/sum(glm.table)
###Type I error
glm.table[1, 2]/sum(glm.table[1, ])
###Type II error
glm.table[2, 1]/sum(glm.table[2, ])
###Power
glm.table[2, 2]/sum(glm.table[2,])
### KNN takes inputs as x (predictors) and y (response) variables
### Since the algorithm utilizes distance, we need to take out categorical variables
### and use the remaining to create the subsets
train.x <- train[4:17]
test.x <- test[4:17]
### response variable should be a vector
train.y = train$churn
test.y = test$churn
kset <- seq(1, 19, by = 2)
error.rate <- rep(0, length(kset))
for(i in kset) {
knn.pred <- knn(train.x, test.x, train.y,k = i)
error.rate[i%/%2 + 1] <- sum(test.y != knn.pred) / length(knn.pred)
}
library(FNN)
library(tidyverse)
for(i in kset) {
knn.pred <- knn(train.x, test.x, train.y,k = i)
error.rate[i%/%2 + 1] <- sum(test.y != knn.pred) / length(knn.pred)
}
rm(list=ls())
library(FNN)
library(tidyverse)
###Get the data
churndata <- read.csv("churndata.csv")
head(churndata)
str(churndata)
###Prepare the data: Clean up, check the data types
churndata$churn <- factor(churndata$churn)
churndata$vmail <- factor(churndata$vmail)
churndata$area <- factor(churndata$area)
sum(is.na(churndata))
churndata <- na.omit(churndata)
## Scale the numerical predictors
churndata[c(4:17)] <- scale(churndata[c(4:17)])
###Create training and test sets
set.seed(123)
churndata <- mutate(churndata, id=1:nrow(churndata))
t_prop <- 0.7
train <- slice_sample(churndata, n = t_prop*nrow(churndata), replace=FALSE)
test <- anti_join(churndata, train, by = 'id')
train <- select(train, -id)
test <- select(test, -id)
set.seed(seed=NULL) # release the seed and set it to be random
### glm function takes the following inputs
### 1. dependent variable ~ independent variables
### 2. data = dataset (whichever data set you would like to build the model with)
### 3. family = binomial
glm.fit <- glm(churn ~ ., data = train, family = binomial)
summary(glm.fit)
### Convert the predicted probabilities into class labels No or Yes
glm.pred <- rep("No", nrow(test))
### type="response" outputs probabilities P(Y = 1|X) instead of the log-odds (logit).
glm.probs <- predict(glm.fit, newdata = test, type = "response")
glm.pred[glm.probs > .5] <- "Yes"
library(FNN)
library(tidyverse)
###Get the data
churndata <- read.csv("churndata.csv")
head(churndata)
str(churndata)
###Prepare the data: Clean up, check the data types
churndata$churn <- factor(churndata$churn)
setwd("~/Documents/Spring 2022/Predictive Analytics/Linear Regression")
###Get the data
churndata <- read.csv("churndata.csv")
head(churndata)
str(churndata)
###Prepare the data: Clean up, check the data types
churndata$churn <- factor(churndata$churn)
churndata$vmail <- factor(churndata$vmail)
churndata$area <- factor(churndata$area)
sum(is.na(churndata))
churndata <- na.omit(churndata)
## Scale the numerical predictors
churndata[c(4:17)] <- scale(churndata[c(4:17)])
###Create training and test sets
set.seed(123)
churndata <- mutate(churndata, id=1:nrow(churndata))
t_prop <- 0.7
train <- slice_sample(churndata, n = t_prop*nrow(churndata), replace=FALSE)
test <- anti_join(churndata, train, by = 'id')
train <- select(train, -id)
test <- select(test, -id)
set.seed(seed=NULL) # release the seed and set it to be random
### glm function takes the following inputs
### 1. dependent variable ~ independent variables
### 2. data = dataset (whichever data set you would like to build the model with)
### 3. family = binomial
glm.fit <- glm(churn ~ ., data = train, family = binomial)
summary(glm.fit)
### Convert the predicted probabilities into class labels No or Yes
glm.pred <- rep("No", nrow(test))
### type="response" outputs probabilities P(Y = 1|X) instead of the log-odds (logit).
glm.probs <- predict(glm.fit, newdata = test, type = "response")
glm.pred[glm.probs > .5] <- "Yes"
### Create the confusion matrix and calculate error rates
glm.table <- table(test$churn, glm.pred, dnn = c("Actual", "Predicted"))
glm.table
### Overall error rate
(glm.table[1, 2]+glm.table[2, 1])/sum(glm.table)
